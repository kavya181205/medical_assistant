# -*- coding: utf-8 -*-
"""medical_project_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pVYuFkTa9lA7nKFGA6hXVYM7B8tb0Dx1
"""


from langchain_core.prompts import PromptTemplate
from langchain_huggingface import ChatHuggingFace,HuggingFaceEmbeddings,HuggingFaceEndpoint
# from langchain_community.document_loaders import PyPDFLoader,DirectoryLoader
# from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.runnables import RunnablePassthrough,RunnableLambda,RunnableParallel
from langchain_core.output_parsers import StrOutputParser

# PDF_METADATA = {
#     "who_dengue": {
#         "disease": "dengue",
#         "knowledge_type": "clinical"
#     },
#     "who_diarrhoea": {
#         "disease": "diarrhea",
#         "knowledge_type": "clinical"
#     },
#     "Asthma Guideline": {
#         "disease": "asthma",
#         "knowledge_type": "clinical"
#     },
#     "cdc_commonCold": {
#         "disease": "common_cold",
#         "knowledge_type": "symptom"
#     },
#     "who_fever": {
#         "disease": "fever",
#         "knowledge_type": "symptom"
#     },
#     "who_mentalHealth": {
#         "disease": "mental_health",
#         "knowledge_type": "policy"
#     },
#     "who_malaria_disease": {
#         "disease": "malaria",
#         "knowledge_type": "clinical"
#     },
#     "who_hiv_disease": {
#         "disease": "hiv",
#         "knowledge_type": "clinical"
#     }
# }

# def add_pdf_metadata(documents, pdf_metadata_map):
#     """
#     Adds disease and knowledge_type metadata to each Document
#     based on the PDF filename.
#     """
#     for doc in documents:
#         source = doc.metadata.get("source", "").lower()

#         # Default values (safe fallback)
#         doc.metadata["disease"] = "general"
#         doc.metadata["knowledge_type"] = "policy"

#         for pdf_name, meta in pdf_metadata_map.items():
#             if pdf_name.lower() in source:
#                 doc.metadata["disease"] = meta["disease"]
#                 doc.metadata["knowledge_type"] = meta["knowledge_type"]
#                 break

#     return documents

# loader=DirectoryLoader(
#     path='/content/drive/MyDrive/medical_recommendation_system/data',
#     glob='*.pdf',
#     loader_cls=PyPDFLoader
# )

# docs=loader.load()

# docs = add_pdf_metadata(docs, PDF_METADATA)

# splitter = RecursiveCharacterTextSplitter(
#     chunk_size=1000,
#     chunk_overlap=200,
#     separators=["\n\n", "\n", ".", " "]
# )

# chunks = splitter.split_documents(docs)

# print(len(chunks))

EMERGENCY_KEYWORDS = [
    "bleeding",
    "unconscious",
    "seizure",
    "difficulty breathing",
    "severe chest pain",
    "very high fever",
    "bp 180",
    "bp 190",
    "fainting",
    "confusion"
]

def is_emergency(query: str) -> bool:
    q = query.lower()
    return any(keyword in q for keyword in EMERGENCY_KEYWORDS)

def emergency_message():
    return (
        "⚠️ This may be a medical emergency. "
        "Please seek immediate medical care or visit the nearest emergency department."
    )

def symptom_sub_router(query: str):
    q = query.lower()

    # Fever-based routing
    if "fever" in q:
        if any(w in q for w in ["headache", "joint pain", "body pain", "rash"]):
            return "dengue"

        if any(w in q for w in ["chills", "sweating", "shivering"]):
            return "malaria"

        return "general_fever"

    # Cold / cough
    if any(w in q for w in ["cold", "cough", "runny nose"]):
        return "common_cold"

    return "general_symptom"

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# db = FAISS.from_documents(chunks, embeddings)

# db.save_local("/content/drive/MyDrive/medical_recommendation_system/vectorstore/faiss2")

from pathlib import Path
from langchain_community.vectorstores import FAISS

BASE_DIR = Path(__file__).resolve().parent

VECTORSTORE_PATH = BASE_DIR / "data" / "vectorstore" / "faiss2"

db = FAISS.load_local(
    VECTORSTORE_PATH,
    embeddings,
    allow_dangerous_deserialization=True
)


dengue_retriever = db.as_retriever(
    search_kwargs={"k": 2, "filter": {"disease": "dengue"}}
)

malaria_retriever = db.as_retriever(
    search_kwargs={"k": 3, "filter": {"disease": "malaria"}}
)

fever_retriever = db.as_retriever(
    search_kwargs={"k": 2, "filter": {"disease": "fever"}}
)

cold_retriever = db.as_retriever(
    search_kwargs={"k": 2, "filter": {"disease": "common_cold"}}
)

clinical_retriever = db.as_retriever(
    search_kwargs={"k": 3,"filter": {"knowledge_type": "clinical"}}
)

symptom_retriever = db.as_retriever(
    search_kwargs={"k": 2,"filter": {"knowledge_type": "symptom"}}
)

policy_retriever = db.as_retriever(
    search_kwargs={"k": 2,"filter": {"knowledge_type": "policy"}}
)

def handle_symptom_query(question: str):
    sub_route = symptom_sub_router(question)

    if sub_route == "dengue":
        return run_rag(question, dengue_retriever)

    if sub_route == "malaria":
        return run_rag(question, malaria_retriever)
        if "could not find" in answer.lower():
            return run_rag_with_retriever(question, fever_retriever)


    if sub_route == "general_fever":
        return run_rag(question, fever_retriever)

    if sub_route == "common_cold":
        return run_rag(question, cold_retriever)

    return (
        "Symptoms vary widely. "
        "Please consult a healthcare professional for proper evaluation."
    )

def route_query(query: str):
  q = query.lower()
  if any(w in q for w in ["chest pain", "bleeding", "unconscious", "bp 180"]):
    return "emergency"

  if any(w in q for w in ["fever", "cold", "vomiting", "pain"]):
    return "symptom"

  if any(w in q for w in ["treat", "manage", "medicine", "therapy"]):
    return "clinical"

  return "policy"

def routed_qa(question: str):

    if is_emergency(question):
        return emergency_message()

    route = route_query(question)

    if route == "symptom":
        return handle_symptom_query(question)

    if route == "clinical":
        return run_rag(
            question,
            clinical_retriever
        )

    if route == "policy":
        return run_rag(
            question,
            policy_retriever
        )

    return (
        "I could not determine the appropriate guidance for this question. "
        "Please consult a healthcare professional."
    )


from dotenv import load_dotenv
load_dotenv()

from huggingface_hub import login
login()

llm = HuggingFaceEndpoint(
    repo_id="openai/gpt-oss-20b",
    task="text-generation",
    temperature=0

)
model= ChatHuggingFace(llm=llm)

prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
You are a medical information assistant providing guidance strictly based on WHO and CDC documents.

CRITICAL RULES (must follow):
- Use ONLY the information present in the provided context.
- Do NOT diagnose diseases.
- Do NOT prescribe medicines, treatments, or dosages.
- Do NOT add external knowledge or assumptions.

If the provided context does NOT contain enough information to answer the question,
reply EXACTLY with:
"I could not find this information in WHO/CDC guidelines."

If symptoms suggest a medical concern but details are limited,
provide general WHO/CDC guidance and advise consulting a healthcare professional.

Context:
{context}

Question:
{question}

Answer requirements:
- Be factual, neutral, and safety-focused
- Avoid certainty words like "definitely" or "always"
- Prefer phrases such as "may indicate", "is associated with", or "can be a sign of"
- Keep the answer concise and structured when helpful

Answer:
"""
)

parser=StrOutputParser()

def format_docs(docs):
    return "\n\n".join(
        f"Source: {d.metadata.get('source', '')}\n{d.page_content}"
        for d in docs
    )

def run_rag(question: str, retriever):
    rag_chain = (
        RunnableParallel({
            "context": retriever | RunnableLambda(format_docs),
            "question": RunnablePassthrough()
        })
        | prompt
        | model
        | parser
    )
    return rag_chain.invoke(question)




